{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUosFop80pZl"
      },
      "source": [
        "#  Indexing and searching image based documents (using ColPali with Qdrant)\n",
        "\n",
        "We can retrieve documents with images such as user guides or old scanned documents. We will use an embedding model for the documents and the queries that supports images. We will also tune the vector database to efficiently store and search these embedding vectors.\n",
        "\n",
        "Here are the steps:\n",
        "* [Creating image collection index](#creating-image-collection-index)\n",
        "* [Searching the image index](#searching-the-image-index)\n",
        "* [Generating a reply based on the retrieved image](#generate-response-with-the-retrieved-images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMA8_V250pZn"
      },
      "source": [
        "## Visual Improvements"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "%cd advanced-rag"
      ],
      "metadata": {
        "id": "HugCEYLWzjhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before installing other requirements, ensure compatible torch and torchvision versions.\n",
        "# The error indicates torchvision 0.24.0 requires torch 2.9.0, while current torch is 2.4.1.\n",
        "# We will uninstall existing torch, torchvision, and torchaudio and then install the required compatible versions.\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch==2.9.0 torchvision==0.24.0+cu121 torchaudio==2.9.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!git clone https://github.com/guyernest/advanced-rag.git\n",
        "%cd advanced-rag\n",
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "nQmsj3er6uYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3ZqU3mvXV_oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GH3GFiUrS8pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhanced Visualisation"
      ],
      "metadata": {
        "id": "l_2OpkptMYBs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a8UUn6n0pZo"
      },
      "outputs": [],
      "source": [
        "from rich.console import Console\n",
        "from rich_theme_manager import Theme, ThemeManager\n",
        "import pathlib\n",
        "\n",
        "theme_dir = pathlib.Path(\"themes\")\n",
        "theme_manager = ThemeManager(theme_dir=theme_dir)\n",
        "dark = theme_manager.get(\"dark\")\n",
        "\n",
        "# Create a console with the dark theme\n",
        "console = Console(theme=dark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA9iS_Uh0pZo"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzqcHISl0pZo"
      },
      "source": [
        "## Creating image collection index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wmBWri10pZo"
      },
      "source": [
        "### Converting PDF files into images\n",
        "\n",
        "We don't want to rely on text extraction from the PDF files, and we want to focus on the visual aspects of the pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGPWTOfD0pZo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pdf2image.pdf2image import convert_from_path, PDFPageCountError\n",
        "\n",
        "def convert_pdfs_to_images(pdf_folder):\n",
        "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
        "    all_images = {}\n",
        "    skipped_pdfs = []\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
        "        try:\n",
        "            images = convert_from_path(pdf_path)\n",
        "            all_images[pdf_file] = images\n",
        "        except PDFPageCountError as e:\n",
        "            print(f\"Skipping '{pdf_file}' due to PDFPageCountError: {e}\")\n",
        "            skipped_pdfs.append(pdf_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping '{pdf_file}' due to an unexpected error: {e}\")\n",
        "            skipped_pdfs.append(pdf_file)\n",
        "\n",
        "    if skipped_pdfs:\n",
        "        print(\"\\nSummary of skipped PDF files:\")\n",
        "        for s_pdf in skipped_pdfs:\n",
        "            print(f\"- {s_pdf}\")\n",
        "    else:\n",
        "        print(\"No PDF files were skipped.\")\n",
        "\n",
        "    return all_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwoPeuVM0pZp"
      },
      "outputs": [],
      "source": [
        "# all_images = convert_pdfs_to_images(\"data/ikea/\")\n",
        "# Install poppler-utils\n",
        "!apt-get install poppler-utils\n",
        "\n",
        "all_images = convert_pdfs_to_images(\"/content/advanced-rag/data/pdf_files/tmp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3D7InX30pZp"
      },
      "outputs": [],
      "source": [
        "console.print(all_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yWOdoL-0pZq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1,2 , figsize=(15, 10))\n",
        "\n",
        "first_pdf_key = next(iter(all_images))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = all_images[first_pdf_key][i]\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5n_OMYu0pZr"
      },
      "outputs": [],
      "source": [
        "from colpali_engine.models import ColPali, ColPaliProcessor\n",
        "import torch\n",
        "\n",
        "\n",
        "# Initialize ColPali model and processor\n",
        "model_name = (\n",
        "    \"vidore/colpali-v1.2\"  # Use the latest version available\n",
        ")\n",
        "colpali_model = ColPali.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cpu\",  # Use \"cuda:0\" for GPU, \"cpu\" for CPU, or \"mps\" for Apple Silicon\n",
        ")\n",
        "colpali_processor = ColPaliProcessor.from_pretrained(\n",
        "    \"vidore/colpaligemma-3b-pt-448-base\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "new_cell_1_protobuf_fix"
      },
      "source": [
        "# Fix for 'MessageFactory' AttributeError: Reinstall protobuf\n",
        "!pip uninstall protobuf -y\n",
        "!pip install protobuf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03fyTS6L0pZs"
      },
      "outputs": [],
      "source": [
        "console.print(colpali_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK6t9qgR0pZs"
      },
      "outputs": [],
      "source": [
        "sample_image = all_images[first_pdf_key][0]\n",
        "with torch.no_grad():\n",
        "    sample_batch = colpali_processor.process_images([sample_image]).to(\n",
        "        colpali_model.device\n",
        "    )\n",
        "    sample_embedding = colpali_model(**sample_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbw7pyUt0pZt"
      },
      "outputs": [],
      "source": [
        "console.print(sample_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O67Rn30t0pZt"
      },
      "outputs": [],
      "source": [
        "from rich.table import Table\n",
        "\n",
        "table = Table(title=\"Document Embedding\")\n",
        "table.add_column(\"Documents\", style=\"cyan\", no_wrap=True)\n",
        "table.add_column(\"Tokens\", style=\"bright_yellow\")\n",
        "table.add_column(\"Vector Size\", style=\"green\")\n",
        "\n",
        "table.add_row(\n",
        "    str(sample_embedding.shape[0]),\n",
        "    str(sample_embedding.shape[1]),\n",
        "    str(sample_embedding.shape[2])\n",
        ")\n",
        "\n",
        "console.print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store embeddings in a persistent Store"
      ],
      "metadata": {
        "id": "tGwUGLbupQZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ChromaDB\n",
        "import os\n",
        "!pip install chromadb\n",
        "import chromadb\n",
        "\n",
        "# Setup Persistent Storage\n",
        "\n",
        "persistent_store = '/content/advanced-rag/data/persistent_store'\n",
        "\n",
        "# Ensure the destination directory exists\n",
        "if not os.path.exists(persistent_store):\n",
        "    os.makedirs(persistent_store) # Creates parent directories if needed\n",
        "%cd '/content/advanced-rag/data/persistent_store'\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"./chroma_db\")"
      ],
      "metadata": {
        "id": "WjdJ3nKIpMf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUTbEjc90pZu"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "   # \":memory:\"\n",
        "    path=\"/content/advanced-rag/data/persistent_store\"\n",
        ")  # Use \":memory:\" for in-memory database or \"path/to/db\" for persistent storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW60ka5D0pZu"
      },
      "outputs": [],
      "source": [
        "vector_size = sample_embedding.shape[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmxxfQ_X0pZu"
      },
      "outputs": [],
      "source": [
        "from qdrant_client.http import models\n",
        "\n",
        "multi_vector_params = models.VectorParams(\n",
        "    size=vector_size,\n",
        "    distance=models.Distance.COSINE,\n",
        "    multivector_config=models.MultiVectorConfig(\n",
        "        comparator=models.MultiVectorComparator.MAX_SIM\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DvpEK4o0pZu"
      },
      "source": [
        "### Reducing vector memory using Quantization\n",
        "\n",
        "We can define a `ScalarQuantizationConfig` and pass it when creating the collection. On the server side, Qdrant will convert the vectors to 8-bit integers, reducing the memory footprint and speeding up the search process. You can also switch on or off the `always_ram` parameter, keeping the vectors in RAM. This will increase performance at the cost of memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t1Ji6up0pZu"
      },
      "outputs": [],
      "source": [
        "scalar_quant = models.ScalarQuantizationConfig(\n",
        "    type=models.ScalarType.INT8,\n",
        "    quantile=0.99,\n",
        "    always_ram=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-69kGnr0pZu"
      },
      "outputs": [],
      "source": [
        "collection_name=\"driver-license\"\n",
        "\n",
        "qdrant_client.recreate_collection(\n",
        "    collection_name=collection_name,  # the name of the collection\n",
        "    on_disk_payload=True,  # store the payload on disk\n",
        "    optimizers_config=models.OptimizersConfigDiff(\n",
        "        indexing_threshold=100\n",
        "    ),  # it can be useful to swith this off when doing a bulk upload and then manually trigger the indexing once the upload is done\n",
        "    vectors_config=models.VectorParams(\n",
        "        size=vector_size,\n",
        "        distance=models.Distance.COSINE,\n",
        "        multivector_config=models.MultiVectorConfig(\n",
        "            comparator=models.MultiVectorComparator.MAX_SIM\n",
        "        ),\n",
        "        quantization_config=models.ScalarQuantization(\n",
        "            scalar=scalar_quant,\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRDFqv4K0pZv"
      },
      "source": [
        "### Upserting the encoded images into the vector database\n",
        "\n",
        "We define a helper function to upload points to Qdrant via the client. We use the stamina library to enable retries in case of network issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7PvpnjB0pZv"
      },
      "outputs": [],
      "source": [
        "import stamina\n",
        "\n",
        "@stamina.retry(on=Exception, attempts=3)\n",
        "def upsert_to_qdrant(batch):\n",
        "    try:\n",
        "        qdrant_client.upsert(\n",
        "            collection_name=collection_name,\n",
        "            points=points,\n",
        "            wait=False,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error during upsert: {e}\")\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nplRrLS0pZw"
      },
      "source": [
        "We will now upload the vectors to qdrant. We do this by creating batches of data, passing it through the ColPali model and then adding the embeddings to a Qdrant `PointStruct`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5BnJtRh0pZw"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 2  # Adjust based on your GPU memory constraints\n",
        "\n",
        "total_images = sum(len(images) for images in all_images.values())\n",
        "\n",
        "# Use tqdm to create a progress bar\n",
        "with tqdm(total=total_images, desc=\"Indexing Progress\") as pbar:\n",
        "    for doc_id, pdf_file in enumerate(all_images.keys()):\n",
        "        for i in range(0, len(all_images[pdf_file]), batch_size):\n",
        "            images = all_images[pdf_file][i : i + batch_size]\n",
        "\n",
        "            # Process and encode images\n",
        "            with torch.no_grad():\n",
        "                batch_images = colpali_processor.process_images(images).to(\n",
        "                    colpali_model.device\n",
        "                )\n",
        "                image_embeddings = colpali_model(**batch_images)\n",
        "\n",
        "            # Prepare points for Qdrant\n",
        "            points = []\n",
        "            for j, embedding in enumerate(image_embeddings):\n",
        "                unique_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{doc_id}.{i + j}\"))\n",
        "                # Convert the embedding to a list of vectors\n",
        "                multivector = embedding.cpu().float().numpy().tolist()\n",
        "                points.append(\n",
        "                    models.PointStruct(\n",
        "                        id=unique_id,\n",
        "                        vector=multivector,  # This is now a list of vectors\n",
        "                        payload={\n",
        "                            \"doc\": pdf_file,\n",
        "                            \"page\": i+j+1\n",
        "                        },  # can also add other metadata/data\n",
        "                    )\n",
        "                )\n",
        "            # Upload points to Qdrant\n",
        "            try:\n",
        "                upsert_to_qdrant(points)\n",
        "            # clown level error handling here ðŸ¤¡\n",
        "            except Exception as e:\n",
        "                print(f\"Error during upsert: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Update the progress bar\n",
        "            pbar.update(batch_size)\n",
        "\n",
        "print(\"Indexing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw3XKF1N0pZw"
      },
      "source": [
        "If you had the indexing off during the upload you can trigger an index by setting a lower indexing threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P3LZnaE0pZw"
      },
      "outputs": [],
      "source": [
        "qdrant_client.update_collection(\n",
        "    collection_name=collection_name,\n",
        "    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=10),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp_q-OIV0pZw"
      },
      "outputs": [],
      "source": [
        "console.print(\n",
        "    qdrant_client\n",
        "    .get_collection(collection_name)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSFV5DLw0pZx"
      },
      "outputs": [],
      "source": [
        "console.print(\n",
        "    qdrant_client\n",
        "    .scroll(\n",
        "        collection_name=collection_name,\n",
        "        limit=20\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Dh_6C80pZx"
      },
      "source": [
        "## Searching the image index\n",
        "\n",
        "Once we uploaded the encoded images to the vector database, we can query it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LovMCahF0pZx"
      },
      "outputs": [],
      "source": [
        "# query_text = \"What is the license number?\"\n",
        "query_text = \"What is Savitha's license number?\"\n",
        "with torch.no_grad():\n",
        "    batch_query = colpali_processor.process_queries([query_text]).to(\n",
        "        colpali_model.device\n",
        "    )\n",
        "    query_embedding = colpali_model(**batch_query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgudayCs0pZx"
      },
      "outputs": [],
      "source": [
        "console.print(query_embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYVYbY8s0pZx"
      },
      "outputs": [],
      "source": [
        "# Convert the query embedding to a list of vectors\n",
        "multivector_query = query_embedding[0].cpu().float().numpy().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi8yBIz-0pZx"
      },
      "outputs": [],
      "source": [
        "search_result = qdrant_client.query_points(\n",
        "    collection_name=collection_name,\n",
        "    query=multivector_query,\n",
        "    limit=3,\n",
        "    timeout=60,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD0VtYjp0pZx"
      },
      "outputs": [],
      "source": [
        "console.print(search_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvrCe2Xs0pZx"
      },
      "source": [
        "### Show the search results images\n",
        "\n",
        "We can display the images that were retrieved by the vector search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTYJfFFT0pZx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract the top 3 images from the search result for display\n",
        "top_images = search_result.points[:6]\n",
        "\n",
        "# Create a figure with subplots for each image\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 10))\n",
        "\n",
        "# Iterate over the top images and plot each one\n",
        "for i, point in enumerate(top_images):\n",
        "    pdf_file = point.payload.get('doc')\n",
        "    page_num = int(point.payload.get('page')) - 1\n",
        "    img = all_images[pdf_file][page_num]\n",
        "    axs[i].imshow(img)\n",
        "    axs[i].set_title(f\"Score: {point.score}, \\n Doc: {pdf_file}\")\n",
        "    axs[i].axis('off')  # Do not display axes for better visualization\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vam87iHc0pZx"
      },
      "source": [
        "## Generate response with the retrieved image(s)\n",
        "\n",
        "In the **A**ugmentation step we encode the retrieved image using base64 and send it as part of the prompt to the generation model, alongside the user's query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW0HDa1r0pZx"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "top_image = search_result.points[0]\n",
        "pdf_file = top_image.payload.get('doc')\n",
        "page_num = int(top_image.payload.get('page')) - 1\n",
        "image = all_images[pdf_file][page_num]\n",
        "display(image)\n",
        "\n",
        "buffered = BytesIO()\n",
        "image.save(buffered, format=\"PNG\")  # You may choose another format if needed\n",
        "img_bytes = buffered.getvalue()\n",
        "\n",
        "image1_media_type = \"image/png\"\n",
        "\n",
        "image1_data = base64.standard_b64encode(img_bytes).decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk8KlWHa0pZ1"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Get the secret value from Colab's secrets manager\n",
        "openai_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=openai_key)\n",
        "\n",
        "message = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # or gpt-4o-mini\n",
        "   messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\",\n",
        "                 \"text\": query_text\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{image1_data}\"\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=2000,\n",
        ")\n",
        "\n",
        "console.print(message)"
      ],
      "metadata": {
        "id": "K1ZaN-GQI29k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxImRt6C0pZ1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}