{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUosFop80pZl"
      },
      "source": [
        "#  Indexing and searching image based documents (using ColPali with Qdrant)\n",
        "\n",
        "We can retrieve documents with images such as user guides or old scanned documents. We will use an embedding model for the documents and the queries that supports images. We will also tune the vector database to efficiently store and search these embedding vectors.\n",
        "\n",
        "Here are the steps:\n",
        "* [Creating image collection index](#creating-image-collection-index)\n",
        "* [Searching the image index](#searching-the-image-index)\n",
        "* [Generating a reply based on the retrieved image](#generate-response-with-the-retrieved-images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMA8_V250pZn"
      },
      "source": [
        "## Visual Improvements"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "%cd advanced-rag"
      ],
      "metadata": {
        "id": "HugCEYLWzjhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before installing other requirements, ensure compatible torch and torchvision versions.\n",
        "# The error indicates torchvision 0.24.0 requires torch 2.9.0, while current torch is 2.4.1.\n",
        "# We will uninstall existing torch, torchvision, and torchaudio and then install the required compatible versions.\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch==2.9.0 torchvision==0.24.0+cu121 torchaudio==2.9.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!git clone https://github.com/guyernest/advanced-rag.git\n",
        "%cd advanced-rag\n",
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "nQmsj3er6uYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3ZqU3mvXV_oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if os.path.ismount('/content/drive'):\n",
        "    print(\"Google Drive is already mounted.\")\n",
        "else:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive is now mounted.\")\n"
      ],
      "metadata": {
        "id": "GH3GFiUrS8pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enhanced Visualisation"
      ],
      "metadata": {
        "id": "l_2OpkptMYBs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a8UUn6n0pZo"
      },
      "outputs": [],
      "source": [
        "from rich.console import Console\n",
        "from rich_theme_manager import Theme, ThemeManager\n",
        "import pathlib\n",
        "\n",
        "theme_dir = pathlib.Path(\"themes\")\n",
        "theme_manager = ThemeManager(theme_dir=theme_dir)\n",
        "dark = theme_manager.get(\"dark\")\n",
        "\n",
        "# Create a console with the dark theme\n",
        "console = Console(theme=dark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA9iS_Uh0pZo"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzqcHISl0pZo"
      },
      "source": [
        "## Creating image collection index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wmBWri10pZo"
      },
      "source": [
        "### Converting PDF files into images\n",
        "\n",
        "We don't want to rely on text extraction from the PDF files, and we want to focus on the visual aspects of the pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGPWTOfD0pZo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pdf2image.pdf2image import convert_from_path, PDFPageCountError\n",
        "\n",
        "def convert_pdfs_to_images(pdf_folder):\n",
        "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
        "    all_images = {}\n",
        "    skipped_pdfs = []\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
        "        try:\n",
        "            images = convert_from_path(pdf_path)\n",
        "            all_images[pdf_file] = images\n",
        "        except PDFPageCountError as e:\n",
        "            print(f\"Skipping '{pdf_file}' due to PDFPageCountError: {e}\")\n",
        "            skipped_pdfs.append(pdf_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping '{pdf_file}' due to an unexpected error: {e}\")\n",
        "            skipped_pdfs.append(pdf_file)\n",
        "\n",
        "    if skipped_pdfs:\n",
        "        print(\"\\nSummary of skipped PDF files:\")\n",
        "        for s_pdf in skipped_pdfs:\n",
        "            print(f\"- {s_pdf}\")\n",
        "    else:\n",
        "        print(\"No PDF files were skipped.\")\n",
        "\n",
        "    return all_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwoPeuVM0pZp"
      },
      "outputs": [],
      "source": [
        "# all_images = convert_pdfs_to_images(\"data/ikea/\")\n",
        "# Install poppler-utils\n",
        "!apt-get install poppler-utils\n",
        "\n",
        "all_images = convert_pdfs_to_images(\"/content/advanced-rag/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3D7InX30pZp"
      },
      "outputs": [],
      "source": [
        "console.print(all_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yWOdoL-0pZq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1,2 , figsize=(15, 10))\n",
        "\n",
        "first_pdf_key = next(iter(all_images))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    img = all_images[first_pdf_key][i]\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5n_OMYu0pZr"
      },
      "outputs": [],
      "source": [
        "from colpali_engine.models import ColPali, ColPaliProcessor\n",
        "import torch\n",
        "\n",
        "\n",
        "# Initialize ColPali model and processor\n",
        "model_name = (\n",
        "    \"vidore/colpali-v1.2\"  # Use the latest version available\n",
        ")\n",
        "colpali_model = ColPali.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda:0\",  # Use \"cuda:0\" for GPU, \"cpu\" for CPU, or \"mps\" for Apple Silicon\n",
        ")\n",
        "colpali_processor = ColPaliProcessor.from_pretrained(\n",
        "    \"vidore/colpaligemma-3b-pt-448-base\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "new_cell_1_protobuf_fix"
      },
      "source": [
        "# Fix for 'MessageFactory' AttributeError: Reinstall protobuf\n",
        "!pip uninstall protobuf -y\n",
        "!pip install protobuf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03fyTS6L0pZs"
      },
      "outputs": [],
      "source": [
        "console.print(colpali_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK6t9qgR0pZs"
      },
      "outputs": [],
      "source": [
        "sample_image = all_images[first_pdf_key][0]\n",
        "with torch.no_grad():\n",
        "    sample_batch = colpali_processor.process_images([sample_image]).to(\n",
        "        colpali_model.device\n",
        "    )\n",
        "    sample_embedding = colpali_model(**sample_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbw7pyUt0pZt"
      },
      "outputs": [],
      "source": [
        "console.print(sample_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O67Rn30t0pZt"
      },
      "outputs": [],
      "source": [
        "from rich.table import Table\n",
        "\n",
        "table = Table(title=\"Document Embedding\")\n",
        "table.add_column(\"Documents\", style=\"cyan\", no_wrap=True)\n",
        "table.add_column(\"Tokens\", style=\"bright_yellow\")\n",
        "table.add_column(\"Vector Size\", style=\"green\")\n",
        "\n",
        "table.add_row(\n",
        "    str(sample_embedding.shape[0]),\n",
        "    str(sample_embedding.shape[1]),\n",
        "    str(sample_embedding.shape[2])\n",
        ")\n",
        "\n",
        "console.print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store embeddings in a persistent Store"
      ],
      "metadata": {
        "id": "tGwUGLbupQZQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUTbEjc90pZu"
      },
      "outputs": [],
      "source": [
        "# from qdrant_client import QdrantClient\n",
        "\n",
        "\n",
        "\n",
        "# from qdrant_client import QdrantClient\n",
        "\n",
        "# # Define path in Drive\n",
        "# qdrant_path = \"/content/drive/MyDrive/RAG_Labs/persistent_storage/qdrant_storage\"\n",
        "\n",
        "# # Initialize client to use disk storage instead of memory\n",
        "# qdrant_client = QdrantClient(\n",
        "#    # \":memory:\"\n",
        "#     path=qdrant_path\n",
        "# )  # Use \":memory:\" for in-memory database or \"path/to/db\" for persistent storage\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "\n",
        "# Replace these with your actual Cloud credentials\n",
        "QDRANT_URL = \"https://f7369634-b961-4d15-ba60-8b230e810658.us-east4-0.gcp.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.pfWUnBNgfw09GSVduWouzMS4d2FoOklapYQawqKq514\"\n",
        "\n",
        "try:\n",
        "    # Initialize the Cloud Client\n",
        "    qdrant_client = QdrantClient(\n",
        "        url=QDRANT_URL,\n",
        "        api_key=QDRANT_API_KEY,\n",
        "    )\n",
        "    print(\"Connected to Qdrant Cloud!\")\n",
        "except Exception as e:\n",
        "    print(f\"Cloud connection failed: {e}\")"
      ],
      "metadata": {
        "id": "f5vKyJGYA3u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KW60ka5D0pZu"
      },
      "outputs": [],
      "source": [
        "vector_size = sample_embedding.shape[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmxxfQ_X0pZu"
      },
      "outputs": [],
      "source": [
        "from qdrant_client.http import models\n",
        "\n",
        "vector_size = 128\n",
        "\n",
        "multi_vector_params = models.VectorParams(\n",
        "    size=vector_size,\n",
        "    distance=models.Distance.COSINE,\n",
        "    multivector_config=models.MultiVectorConfig(\n",
        "        comparator=models.MultiVectorComparator.MAX_SIM\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DvpEK4o0pZu"
      },
      "source": [
        "### Reducing vector memory using Quantization\n",
        "\n",
        "We can define a `ScalarQuantizationConfig` and pass it when creating the collection. On the server side, Qdrant will convert the vectors to 8-bit integers, reducing the memory footprint and speeding up the search process. You can also switch on or off the `always_ram` parameter, keeping the vectors in RAM. This will increase performance at the cost of memory usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t1Ji6up0pZu"
      },
      "outputs": [],
      "source": [
        "scalar_quant = models.ScalarQuantizationConfig(\n",
        "    type=models.ScalarType.INT8,\n",
        "    quantile=0.99,\n",
        "    always_ram=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-69kGnr0pZu"
      },
      "outputs": [],
      "source": [
        "collection_name=\"rag_documents_v2\"\n",
        "\n",
        "qdrant_client.recreate_collection(\n",
        "    collection_name=collection_name,  # the name of the collection\n",
        "    on_disk_payload=True,  # store the payload on disk\n",
        "    optimizers_config=models.OptimizersConfigDiff(\n",
        "        indexing_threshold=100\n",
        "    ),  # it can be useful to swith this off when doing a bulk upload and then manually trigger the indexing once the upload is done\n",
        "    vectors_config=models.VectorParams(\n",
        "        size=vector_size,\n",
        "        distance=models.Distance.COSINE,\n",
        "        multivector_config=models.MultiVectorConfig(\n",
        "            comparator=models.MultiVectorComparator.MAX_SIM\n",
        "        ),\n",
        "        quantization_config=models.ScalarQuantization(\n",
        "            scalar=scalar_quant,\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRDFqv4K0pZv"
      },
      "source": [
        "### Upserting the encoded images into the vector database\n",
        "\n",
        "We define a helper function to upload points to Qdrant via the client. We use the stamina library to enable retries in case of network issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7PvpnjB0pZv"
      },
      "outputs": [],
      "source": [
        "import stamina\n",
        "\n",
        "@stamina.retry(on=Exception, attempts=3)\n",
        "def upsert_to_qdrant(batch):\n",
        "    try:\n",
        "        qdrant_client.upsert(\n",
        "            collection_name=collection_name,\n",
        "            points=points,\n",
        "            wait=False,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error during upsert: {e}\")\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nplRrLS0pZw"
      },
      "source": [
        "We will now upload the vectors to qdrant. We do this by creating batches of data, passing it through the ColPali model and then adding the embeddings to a Qdrant `PointStruct`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5BnJtRh0pZw"
      },
      "outputs": [],
      "source": [
        "# import uuid\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# batch_size = 2  # Adjust based on your GPU memory constraints\n",
        "\n",
        "# total_images = sum(len(images) for images in all_images.values())\n",
        "\n",
        "# # Use tqdm to create a progress bar\n",
        "# with tqdm(total=total_images, desc=\"Indexing Progress\") as pbar:\n",
        "#     for doc_id, pdf_file in enumerate(all_images.keys()):\n",
        "#         for i in range(0, len(all_images[pdf_file]), batch_size):\n",
        "#             images = all_images[pdf_file][i : i + batch_size]\n",
        "\n",
        "#             # Process and encode images\n",
        "#             with torch.no_grad():\n",
        "#                 batch_images = colpali_processor.process_images(images).to(\n",
        "#                     colpali_model.device\n",
        "#                 )\n",
        "#                 image_embeddings = colpali_model(**batch_images)\n",
        "\n",
        "#             # Prepare points for Qdrant\n",
        "#             points = []\n",
        "#             for j, embedding in enumerate(image_embeddings):\n",
        "#                 unique_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{doc_id}.{i + j}\"))\n",
        "#                 # Convert the embedding to a list of vectors\n",
        "#                 multivector = embedding.cpu().float().numpy().tolist()\n",
        "#                 points.append(\n",
        "#                     models.PointStruct(\n",
        "#                         id=unique_id,\n",
        "#                         vector=multivector,  # This is now a list of vectors\n",
        "#                         payload={\n",
        "#                             \"doc\": pdf_file,\n",
        "#                             \"page\": i+j+1\n",
        "#                         },  # can also add other metadata/data\n",
        "#                     )\n",
        "#                 )\n",
        "#             # Upload points to Qdrant\n",
        "#             try:\n",
        "#                 upsert_to_qdrant(points)\n",
        "#             # clown level error handling here ðŸ¤¡\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error during upsert: {e}\")\n",
        "#                 continue\n",
        "\n",
        "#             # Update the progress bar\n",
        "#             pbar.update(batch_size)\n",
        "\n",
        "# print(\"Indexing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import uuid\n",
        "# import os\n",
        "# from tqdm import tqdm\n",
        "# import torch\n",
        "\n",
        "# # 1. Define your storage path on G Drive\n",
        "# image_storage_root = \"/content/drive/MyDrive/RAG_Labs/persistent_storage/qdrant_storage\"\n",
        "# os.makedirs(image_storage_root, exist_ok=True)\n",
        "\n",
        "# batch_size = 2\n",
        "# total_images = sum(len(images) for images in all_images.values())\n",
        "\n",
        "# with tqdm(total=total_images, desc=\"Indexing Progress\") as pbar:\n",
        "#     for doc_id, pdf_file in enumerate(all_images.keys()):\n",
        "#         # Create a subfolder for each PDF to keep things organized\n",
        "#         doc_folder = os.path.join(image_storage_root, f\"doc_{doc_id}\")\n",
        "#         os.makedirs(doc_folder, exist_ok=True)\n",
        "\n",
        "#         for i in range(0, len(all_images[pdf_file]), batch_size):\n",
        "#             images = all_images[pdf_file][i : i + batch_size]\n",
        "\n",
        "#             # --- NEW: Save images and track paths ---\n",
        "#             batch_paths = []\n",
        "#             for j, img in enumerate(images):\n",
        "#                 page_num = i + j + 1\n",
        "#                 image_path = os.path.join(doc_folder, f\"page_{page_num}.png\")\n",
        "#                 img.save(image_path, \"PNG\")\n",
        "#                 batch_paths.append(image_path)\n",
        "#             # ----------------------------------------\n",
        "\n",
        "#             with torch.no_grad():\n",
        "#                 batch_images = colpali_processor.process_images(images).to(\n",
        "#                     colpali_model.device\n",
        "#                 )\n",
        "#                 image_embeddings = colpali_model(**batch_images)\n",
        "\n",
        "#             points = []\n",
        "#             for j, embedding in enumerate(image_embeddings):\n",
        "#                 unique_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{doc_id}.{i + j}\"))\n",
        "#                 multivector = embedding.cpu().float().numpy().tolist()\n",
        "\n",
        "#                 points.append(\n",
        "#                     models.PointStruct(\n",
        "#                         id=unique_id,\n",
        "#                         vector=multivector,\n",
        "#                         payload={\n",
        "#                             \"doc\": pdf_file,\n",
        "#                             \"page\": i + j + 1,\n",
        "#                             \"file_path\": batch_paths[j] # Added the path here\n",
        "#                         },\n",
        "#                     )\n",
        "#                 )\n",
        "\n",
        "#             try:\n",
        "#                 upsert_to_qdrant(points)\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error during upsert: {e}\")\n",
        "#                 continue\n",
        "\n",
        "#             pbar.update(len(images)) # Use len(images) to be precise with the last batch\n",
        "\n",
        "# print(\"Indexing complete!\")"
      ],
      "metadata": {
        "id": "_V000G1jVdXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---##--- Last used successfully - put embeddings  had padded vectors ------##----\n",
        "\n",
        "â¬â¬â¬â¬â¬â¬â¬"
      ],
      "metadata": {
        "id": "luCkB-ZV2hGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import uuid\n",
        "# import base64\n",
        "# from io import BytesIO\n",
        "# from tqdm import tqdm\n",
        "# import torch\n",
        "# import stamina\n",
        "\n",
        "# # Configuration for Qdrant Cloud\n",
        "# # qdrant_client should be initialized with (url=\"...\", api_key=\"...\")\n",
        "\n",
        "# batch_size = 2\n",
        "# total_images = sum(len(images) for images in all_images.values())\n",
        "\n",
        "# with tqdm(total=total_images, desc=\"Indexing Progress\") as pbar:\n",
        "#     for doc_id, pdf_file in enumerate(all_images.keys()):\n",
        "#         for i in range(0, len(all_images[pdf_file]), batch_size):\n",
        "#             images = all_images[pdf_file][i : i + batch_size]\n",
        "\n",
        "#             # 1. Generate ColPali embeddings\n",
        "#             with torch.no_grad():\n",
        "#                 batch_images = colpali_processor.process_images(images).to(\n",
        "#                     colpali_model.device\n",
        "#                 )\n",
        "#                 image_embeddings = colpali_model(**batch_images)\n",
        "\n",
        "\n",
        "\n",
        "#             # 2. Prepare points with Base64 payloads\n",
        "#             points = []\n",
        "#             for j, embedding in enumerate(image_embeddings):\n",
        "#                 # --- NEW: Convert PIL image to Base64 string ---\n",
        "#                 buffered = BytesIO()\n",
        "#                 images[j].save(buffered, format=\"PNG\")\n",
        "#                 base64_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "#                 # -----------------------------------------------\n",
        "\n",
        "#                 unique_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{doc_id}.{i + j}\"))\n",
        "#                 multivector = embedding.cpu().float().numpy().tolist()\n",
        "\n",
        "#                 points.append(\n",
        "#                     models.PointStruct(\n",
        "#                         id=unique_id,\n",
        "#                         vector=multivector,\n",
        "#                         payload={\n",
        "#                             \"doc\": pdf_file,\n",
        "#                             \"page\": i + j + 1,\n",
        "#                             \"base64_image\": base64_string  # Image data is now in the cloud\n",
        "#                         },\n",
        "#                     )\n",
        "#                 )\n",
        "\n",
        "#            # 3. Upsert to Qdrant Cloud\n",
        "#             try:\n",
        "#                 qdrant_client.upsert(\n",
        "#                     collection_name=\"rag_documents_v1\",\n",
        "#                     points=points\n",
        "#                 )\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error during upsert: {e}\")\n",
        "#                 continue\n",
        "\n",
        "#             pbar.update(len(images))\n",
        "\n",
        "# print(\"Indexing complete! Images are now stored in Qdrant Cloud.\")"
      ],
      "metadata": {
        "id": "-1LcmrvCD7c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import stamina\n",
        "\n",
        "# Configuration for Qdrant Cloud\n",
        "# qdrant_client should be initialized with (url=\"...\", api_key=\"...\")\n",
        "\n",
        "batch_size = 2\n",
        "total_images = sum(len(images) for images in all_images.values())\n",
        "print(\"Total Images :\",str(total_images))\n",
        "#\n",
        "\n",
        "with tqdm(total=total_images, desc=\"Indexing Progress\") as pbar:\n",
        "    for doc_id, pdf_file in enumerate(all_images.keys()):\n",
        "        for i in range(0, len(all_images[pdf_file]), batch_size):\n",
        "            images = all_images[pdf_file][i : i + batch_size]\n",
        "\n",
        "            # 1. Generate ColPali embeddings\n",
        "            with torch.no_grad():\n",
        "              batch_images = colpali_processor.process_images(images).to(colpali_model.device)\n",
        "              image_embeddings = colpali_model(**batch_images)\n",
        "\n",
        "              # Access the attention mask to filter out padding tokens\n",
        "              # This is critical for finding small details like children's names\n",
        "              mask = batch_images.attention_mask\n",
        "\n",
        "\n",
        "\n",
        "            # 2. Prepare points with Base64 payloads\n",
        "            points = []\n",
        "\n",
        "            # If image_embeddings is a list, we iterate through it directly\n",
        "            for j, embedding in enumerate(image_embeddings):\n",
        "                # Determine the number of non-padding tokens for this specific image\n",
        "                actual_num_patches = mask[j].sum().item()\n",
        "\n",
        "                # Filter the embedding to only include 'real' visual patches before converting to list\n",
        "                # This prevents 'diluting' the search score with empty padding vectors\n",
        "                filtered_embedding = embedding[:actual_num_patches].cpu().float().numpy().tolist()\n",
        "\n",
        "                # --- Convert PIL image to Base64 string ---\n",
        "                buffered = BytesIO()\n",
        "                images[j].save(buffered, format=\"PNG\")\n",
        "                base64_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "                unique_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{doc_id}.{i + j}\"))\n",
        "\n",
        "                points.append(\n",
        "                    models.PointStruct(\n",
        "                        id=unique_id,\n",
        "                        vector=filtered_embedding, # Use the filtered multivector\n",
        "                        payload={\n",
        "                            \"doc\": pdf_file,\n",
        "                            \"page\": i + j + 1,\n",
        "                            \"base64_image\": base64_string\n",
        "                        },\n",
        "                    )\n",
        "                )\n",
        "\n",
        "           # 3. Upsert to Qdrant Cloud\n",
        "            try:\n",
        "                qdrant_client.upsert(\n",
        "                    collection_name=\"rag_documents_v2\",\n",
        "                    points=points\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error during upsert: {e}\")\n",
        "                continue\n",
        "\n",
        "            pbar.update(len(images))\n",
        "\n",
        "print(\"Indexing complete! Images are now stored in Qdrant Cloud.\")"
      ],
      "metadata": {
        "id": "Izmr45Kj3Bvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw3XKF1N0pZw"
      },
      "source": [
        "If you had the indexing off during the upload you can trigger an index by setting a lower indexing threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7P3LZnaE0pZw"
      },
      "outputs": [],
      "source": [
        "qdrant_client.update_collection(\n",
        "    collection_name=collection_name,\n",
        "    optimizer_config=models.OptimizersConfigDiff(indexing_threshold=10),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp_q-OIV0pZw"
      },
      "outputs": [],
      "source": [
        "console.print(\n",
        "    qdrant_client\n",
        "    .get_collection(\"rag_documents_v2\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSFV5DLw0pZx"
      },
      "outputs": [],
      "source": [
        "console.print(\n",
        "    qdrant_client\n",
        "    .scroll(\n",
        "        collection_name=\"rag_documents_v2\",\n",
        "        limit=2\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "import torch\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct\n",
        "import base64\n",
        "import uuid\n",
        "from io import BytesIO\n",
        "\n",
        "# 1. Load your image\n",
        "image_path = \"/content/drive/MyDrive/RAG_Labs/pdf_files/medicare_files/savi_medicare_img.jpg\"\n",
        "image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# 2. Encode to Base64 (to store in Qdrant payload so the UI can display it later)\n",
        "buffered = BytesIO()\n",
        "image.save(buffered, format=\"PNG\")\n",
        "base64_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "# 3. Generate ColPali Embeddings\n",
        "# Use the same processor/model currently in your 'models' dictionary\n",
        "with torch.no_grad():\n",
        "    batch_images = colpali_processor.process_images([image]).to(colpali_model.device)\n",
        "    image_embeddings = colpali_model(**batch_images)\n",
        "    # Convert to list for Qdrant and flatten the list\n",
        "    vector = image_embeddings.cpu().float().numpy().tolist()[0]\n",
        "\n",
        "\n",
        "# 4. Upsert to Qdrant\n",
        "unique_id = str(uuid.uuid4())\n",
        "try:\n",
        "      qdrant_client.upsert(\n",
        "          collection_name=\"rag_documents_v2\",\n",
        "          points=[\n",
        "              PointStruct(\n",
        "                  id=unique_id, # A unique integer or UUID\n",
        "                  vector=vector,\n",
        "                  payload={\n",
        "                      \"doc\": \"manual_upload\",\n",
        "                      \"page\": 1,\n",
        "                      \"base64_image\": base64_string\n",
        "                  }\n",
        "              )\n",
        "          ]\n",
        "      )\n",
        "except Exception as e:\n",
        "                print(f\"Error during upsert: {e}\")"
      ],
      "metadata": {
        "id": "OeYqkRGQGyMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pdf2image import convert_from_path\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from colpali_engine.models import ColPali\n",
        "#from colpali_engine.processor import ColPaliProcessor\n",
        "from colpali_engine.models import ColPali, ColPaliProcessor\n",
        "import base64\n",
        "import uuid\n",
        "from io import BytesIO\n",
        "\n",
        "def process_and_upsert_pdf(pdf_path: str):\n",
        "    # 2. Convert PDF to Images (Generates one PIL image per page)\n",
        "    # Using 150 DPI is a good balance for ColPali\n",
        "    images = convert_from_path(pdf_path, dpi=150)\n",
        "\n",
        "    points = []\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        page_num = i + 1\n",
        "        print(f\"Processing page {page_num}...\")\n",
        "\n",
        "        # 3. Generate ColPali Embeddings\n",
        "        with torch.no_grad():\n",
        "            batch_images = colpali_processor.process_images([image]).to(colpali_model.device)\n",
        "            image_embeddings = colpali_model(**batch_images)\n",
        "\n",
        "            #Access the attention mask\n",
        "            mask = batch_images.attention_mask\n",
        "            # Remove batch dim and move to CPU for Qdrant\n",
        "            #multivector = image_embeddings[0].cpu().float().numpy().tolist()\n",
        "\n",
        "        # 2. Prepare points with Base64 payloads\n",
        "            points = []\n",
        "\n",
        "            # If image_embeddings is a list, we iterate through it directly\n",
        "            for j, embedding in enumerate(image_embeddings):\n",
        "                # Determine the number of non-padding tokens for this specific image\n",
        "                actual_num_patches = mask[j].sum().item()\n",
        "\n",
        "                # Filter the embedding to only include 'real' visual patches before converting to list\n",
        "                # This prevents 'diluting' the search score with empty padding vectors\n",
        "                filtered_embedding = embedding[:actual_num_patches].cpu().float().numpy().tolist()\n",
        "\n",
        "                # --- Convert PIL image to Base64 string ---\n",
        "                buffered = BytesIO()\n",
        "                images[j].save(buffered, format=\"PNG\")\n",
        "                base64_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "                unique_id = str(uuid.uuid4())\n",
        "\n",
        "                points.append(\n",
        "                    models.PointStruct(\n",
        "                        id=unique_id,\n",
        "                        vector=filtered_embedding, # Use the filtered multivector\n",
        "                        payload={\n",
        "                            \"doc\": \"Ishya_Passport\",\n",
        "                            \"page\": i + j + 1,\n",
        "                            \"base64_image\": base64_string\n",
        "                        },\n",
        "                    )\n",
        "                )\n",
        "\n",
        "           # 3. Upsert to Qdrant Cloud\n",
        "            try:\n",
        "                qdrant_client.upsert(\n",
        "                    collection_name=\"rag_documents_v2\",\n",
        "                    points=points\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error during upsert: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"Finished upserting {pdf_path}\")\n",
        "\n",
        "# Run the pipeline\n",
        "process_and_upsert_pdf(\"/content/advanced-rag/data/Ishya Passport 2026.pdf\")\n"
      ],
      "metadata": {
        "id": "KMbJDm5rcXW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Dh_6C80pZx"
      },
      "source": [
        "## Searching the image index\n",
        "\n",
        "Once we uploaded the encoded images to the vector database, we can query it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LovMCahF0pZx"
      },
      "outputs": [],
      "source": [
        "query_text = \"Does Ishya have a passport\"\n",
        "#query_text = \"What is Ishya's passport number? Use ONLY the provided context to answer. If the answer is not in the context, say 'Passport number not found'. Do not use prior knowledge\"\n",
        "with torch.no_grad():\n",
        "    batch_query = colpali_processor.process_queries([query_text]).to(\n",
        "        colpali_model.device\n",
        "    )\n",
        "    query_embedding = colpali_model(**batch_query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgudayCs0pZx"
      },
      "outputs": [],
      "source": [
        "console.print(query_embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYVYbY8s0pZx"
      },
      "outputs": [],
      "source": [
        "# Convert the query embedding to a list of vectors\n",
        "multivector_query = query_embedding[0].cpu().float().numpy().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi8yBIz-0pZx"
      },
      "outputs": [],
      "source": [
        "search_result = qdrant_client.query_points(\n",
        "    collection_name=\"rag_documents_v2\",\n",
        "    query=multivector_query,\n",
        "    limit=3,\n",
        "    timeout=60,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_unique_doc_names(collection_name=\"rag_documents_v2\"):\n",
        "    doc_names = set()\n",
        "    next_page_offset = None\n",
        "\n",
        "    while True:\n",
        "        # Scroll through the collection\n",
        "        points, next_page_offset = qdrant_client.scroll(\n",
        "            collection_name=collection_name,\n",
        "            limit=100, # Adjust batch size as needed\n",
        "            with_payload=[\"doc\"], # Only fetch the 'doc' field to save bandwidth\n",
        "            with_vectors=False,   # We don't need the vectors for this\n",
        "            offset=next_page_offset\n",
        "        )\n",
        "\n",
        "        for point in points:\n",
        "            if \"doc\" in point.payload:\n",
        "                doc_names.add(point.payload[\"doc\"])\n",
        "\n",
        "        # If next_page_offset is None, we've reached the end\n",
        "        if next_page_offset is None:\n",
        "            break\n",
        "\n",
        "    return list(doc_names)\n",
        "\n",
        "# Usage\n",
        "unique_docs = get_unique_doc_names()\n",
        "print(f\"Documents found: {unique_docs}\")"
      ],
      "metadata": {
        "id": "FqzjN1brUSID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvrCe2Xs0pZx"
      },
      "source": [
        "### Show the search results images\n",
        "\n",
        "We can display the images that were retrieved by the vector search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTYJfFFT0pZx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from base64 import b64decode\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Extract the top images from the search result for display\n",
        "top_images = search_result.points[:6] # Adjust limit as needed, up to 6 for 2x3 grid\n",
        "\n",
        "# Determine number of images to display and create subplots\n",
        "num_images = len(top_images)\n",
        "if num_images == 0:\n",
        "    print(\"No images to display.\")\n",
        "else:\n",
        "    # Calculate rows and columns for subplot grid\n",
        "    cols = 3 # Max 3 columns for better readability\n",
        "    rows = (num_images + cols - 1) // cols # Calculate rows needed\n",
        "\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
        "    axs = axs.flatten() # Flatten the array for easy iteration\n",
        "\n",
        "    # Iterate over the top images and plot each one\n",
        "    for i, point in enumerate(top_images):\n",
        "        base64_string = point.payload.get('base64_image')\n",
        "        if base64_string:\n",
        "            image_data = b64decode(base64_string)\n",
        "            img = Image.open(BytesIO(image_data))\n",
        "            axs[i].imshow(img)\n",
        "            pdf_file = point.payload.get('doc', 'N/A')\n",
        "            page_num = point.payload.get('page', 'N/A')\n",
        "            axs[i].set_title(f\"Score: {point.score:.2f}\\nDoc: {pdf_file}, Page: {page_num}\")\n",
        "            axs[i].axis('off')  # Do not display axes for better visualization\n",
        "        else:\n",
        "            axs[i].set_title(\"Image not found\")\n",
        "            axs[i].axis('off')\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(i + 1, len(axs)):\n",
        "        fig.delaxes(axs[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vam87iHc0pZx"
      },
      "source": [
        "## Generate response with the retrieved image(s)\n",
        "\n",
        "In the **A**ugmentation step we encode the retrieved image using base64 and send it as part of the prompt to the generation model, alongside the user's query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW0HDa1r0pZx"
      },
      "outputs": [],
      "source": [
        "# import base64\n",
        "# from io import BytesIO\n",
        "\n",
        "# top_image = search_result.points[0]\n",
        "# pdf_file = top_image.payload.get('doc')\n",
        "# page_num = int(top_image.payload.get('page')) - 1\n",
        "# image = all_images[pdf_file][page_num]\n",
        "# display(image)\n",
        "\n",
        "# buffered = BytesIO()\n",
        "# image.save(buffered, format=\"PNG\")  # You may choose another format if needed\n",
        "# img_bytes = buffered.getvalue()\n",
        "\n",
        "# image1_media_type = \"image/png\"\n",
        "\n",
        "# image1_data = base64.standard_b64encode(img_bytes).decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import base64\n",
        "# from io import BytesIO\n",
        "# from PIL import Image\n",
        "\n",
        "# # 1. Get the top result from Qdrant\n",
        "# top_image = search_result.points[0]\n",
        "\n",
        "# # 2. Extract the file path and metadata from the payload [cite: 24, 25]\n",
        "# # Note: We now use 'file_path' which we added to the payload earlier\n",
        "# image_path = top_image.payload.get('file_path')\n",
        "# pdf_file = top_image.payload.get('doc') #[cite: 28]\n",
        "# page_num = top_image.payload.get('page')# [cite: 28]\n",
        "\n",
        "# print(f\"Loading image from: {image_path}\")\n",
        "\n",
        "# # 3. Load the image directly from Google Drive\n",
        "# # This replaces: image = all_images[pdf_file][page_num]\n",
        "# try:\n",
        "#     image = Image.open(image_path)\n",
        "#     display(image)# [cite: 28]\n",
        "\n",
        "#     # 4. Prepare the image for the Vision Model (Base64 encoding)\n",
        "#     buffered = BytesIO() #[cite: 28]\n",
        "#     image.save(buffered, format=\"PNG\") #[cite: 28]\n",
        "#     img_bytes = buffered.getvalue()# [cite: 28]\n",
        "\n",
        "#     image1_media_type = \"image/png\"# [cite: 28]\n",
        "#     image1_data = base64.standard_b64encode(img_bytes).decode(\"utf-8\")# [cite: 28]\n",
        "\n",
        "#     print(\"Image encoded successfully for LLM augmentation.\")\n",
        "\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"Error: The image at {image_path} was not found. Is Google Drive mounted?\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "f1MJekwCdX_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialise Copali and Qadrant"
      ],
      "metadata": {
        "id": "QmvrSdZdCFCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.exceptions import UnexpectedResponse\n",
        "from colpali_engine.models import ColPali, ColPaliProcessor\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "\n",
        "#####################################################################\n",
        "#   Initializing Qdrant Cloud\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "# Replace these with your actual Cloud credentials\n",
        "QDRANT_URL = \"https://f7369634-b961-4d15-ba60-8b230e810658.us-east4-0.gcp.cloud.qdrant.io\"\n",
        "\n",
        "try:\n",
        "    # Initialize the Cloud Client\n",
        "    qdrant_client = QdrantClient(\n",
        "        url=QDRANT_URL,\n",
        "        api_key=userdata.get('QDRANT_API_KEY'),\n",
        "    )\n",
        "    print(\"Connected to Qdrant Cloud!\")\n",
        "except Exception as e:\n",
        "    print(f\"Cloud connection failed: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#   Initializing Colpali\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "# Initialize ColPali model and processor\n",
        "model_name = (\n",
        "    \"vidore/colpali-v1.2\"  # Use the latest version available\n",
        ")\n",
        "colpali_model = ColPali.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda:0\",  # Use \"cuda:0\" for GPU, \"cpu\" for CPU, or \"mps\" for Apple Silicon\n",
        ")\n",
        "colpali_processor = ColPaliProcessor.from_pretrained(\n",
        "    \"vidore/colpaligemma-3b-pt-448-base\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "aocN9g5GCMJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import base64\n",
        "# from io import BytesIO\n",
        "# from PIL import Image\n",
        "# import torch\n",
        "\n",
        "# def run_visual_rag(query_text, model, processor, client, collection_name=\"colpali_collection\"):\n",
        "#     \"\"\"\n",
        "#     Complete RAG pipeline: Query -> Search -> Image Retrieval -> Base64 Encoding\n",
        "#     \"\"\"\n",
        "#     # 1. Encode the text query\n",
        "#     with torch.no_grad():\n",
        "#         # ColPali uses a specific processor for queries\n",
        "#         batch_query = processor.process_queries([query_text]).to(model.device)\n",
        "#         query_embedding = model(**batch_query).cpu().float().numpy().tolist()[0]\n",
        "\n",
        "#     # 2. Search Qdrant\n",
        "#     search_result = client.search(\n",
        "#         collection_name=collection_name,\n",
        "#         query_vector=query_embedding,\n",
        "#         limit=1  # Get the top relevant page\n",
        "#     )\n",
        "\n",
        "#     if not search_result:\n",
        "#         return \"No relevant documents found.\", None\n",
        "\n",
        "#     top_hit = search_result[0]\n",
        "\n",
        "#     # 3. Retrieve metadata from payload [cite: 5, 8]\n",
        "#     image_path = top_hit.payload.get('file_path')\n",
        "#     pdf_name = top_hit.payload.get('doc')\n",
        "#     page_num = top_hit.payload.get('page')\n",
        "\n",
        "#     # 4. Load the image from G Drive\n",
        "#     try:\n",
        "#         image = Image.open(image_path)\n",
        "\n",
        "#         # 5. Convert to Base64 for the Vision LLM\n",
        "#         buffered = BytesIO()\n",
        "#         image.save(buffered, format=\"PNG\")\n",
        "#         img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "#         print(f\"Match found! Source: {pdf_name}, Page: {page_num}\")\n",
        "\n",
        "#         return {\n",
        "#             \"answer_image\": image, # For displaying in notebook\n",
        "#             \"base64_data\": img_str, # For the LLM prompt\n",
        "#             \"metadata\": top_hit.payload\n",
        "#         }\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error loading image: {e}\")\n",
        "#         return None, None\n",
        "\n",
        "# # --- How to use it ---\n",
        "# # result = run_visual_rag(\"What is the total revenue in the chart?\", colpali_model, colpali_processor, client)\n",
        "# # if result:\n",
        "# #     display(result['answer_image'])"
      ],
      "metadata": {
        "id": "q7zuOdsTB56H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from qdrant_client import models\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "def run_visual_rag_full(query_text, colpali_model, colpali_processor, qdrant_client):\n",
        "    \"\"\"\n",
        "    Full pipeline:\n",
        "    1. Embed query (ColPali)\n",
        "    2. Search Qdrant (Multi-vector + Quantization)\n",
        "    3. Augment with GPT-4o Vision\n",
        "    \"\"\"\n",
        "\n",
        "    # --- STEP 1: ENCODE THE TEXT QUERY ---\n",
        "    with torch.no_grad():\n",
        "        # Process the query through ColPali\n",
        "        batch_query = colpali_processor.process_queries([query_text]).to(colpali_model.device)\n",
        "        # Convert the multi-vector output to a list of lists for Qdrant\n",
        "        #query_embeddings = colpali_model(**batch_query).cpu().float().numpy().tolist()[0]\n",
        "        query_embedding = colpali_model(**batch_query)\n",
        "        single_query_vector = query_embedding[0].cpu().float().numpy().tolist()\n",
        "\n",
        "    # --- STEP 2: SEARCH QDRANT CLOUD ---\n",
        "    # Using query_points to handle the Scalar Quantization and Multi-vector configuration\n",
        "    try:\n",
        "        search_result = qdrant_client.query_points(\n",
        "            collection_name=\"rag_documents_v2\",\n",
        "            query=single_query_vector,\n",
        "            limit=3,\n",
        "            with_payload=True # Crucial to retrieve the 'base64_image' from disk\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Search failed: {e}\")\n",
        "        return None\n",
        "\n",
        "    if not search_result.points:\n",
        "        print(\"No matches found.\")\n",
        "        return None\n",
        "\n",
        "    # Extract top match and payload\n",
        "    top_hit = search_result.points[0]\n",
        "    base64_image = top_hit.payload.get('base64_image')\n",
        "    metadata = {\n",
        "        \"doc\": top_hit.payload.get('doc'),\n",
        "        \"page\": top_hit.payload.get('page')\n",
        "    }\n",
        "\n",
        "    print(f\"Match found! Source: {metadata['doc']}, Page: {metadata['page']}\")\n",
        "\n",
        "    # --- STEP 3: AUGMENT WITH GPT-4o VISION ---\n",
        "    # Initializing OpenAI client using your stored key\n",
        "    openai_client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant that answers questions based on the provided image context.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": f\"Question: {query_text}\"},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/png;base64,{base64_image}\" # Using png as per your upsert format\n",
        "                        },\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        max_tokens=500,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"answer\": response.choices[0].message.content,\n",
        "        \"metadata\": metadata\n",
        "    }\n",
        "\n",
        "# --- EXECUTION ---\n",
        "query = \"When is Ishya's medicare expiring\"\n",
        "result = run_visual_rag_full(query, colpali_model, colpali_processor, qdrant_client)\n",
        "\n",
        "if result:\n",
        "    print(\"\\n--- GPT-4o Response ---\")\n",
        "    print(result['answer'])"
      ],
      "metadata": {
        "id": "RjDJ5g88JlGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk8KlWHa0pZ1"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Get the secret value from Colab's secrets manager\n",
        "openai_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sTVG42RfJsKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=openai_key)\n",
        "system_instructions = \"You are a helpful assistant that answers questions based on the provided context.\"\n",
        "\n",
        "message = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # or gpt-4o-mini\n",
        "   messages=[\n",
        "       {\"role\": \"system\", \"content\": system_instructions},\n",
        "       #{\"role\": \"user\", \"content\": \"Can you retrieve text from images that I pass as content in the query?\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\",\n",
        "                 \"text\": query_text\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{image1_data}\"\n",
        "                    },\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=2000,\n",
        ")\n",
        "\n",
        "console.print(message)"
      ],
      "metadata": {
        "id": "K1ZaN-GQI29k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxImRt6C0pZ1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}