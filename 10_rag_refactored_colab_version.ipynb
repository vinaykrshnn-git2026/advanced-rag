{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXjkzzWS4xpY7xQUrz3CaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinaykrshnn-git2026/advanced-rag/blob/main/10_rag_refactored_colab_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced RAG refactored (Search and Augment)\n",
        "\n",
        "Refer to 08_multimodal_pdf.ipynb for embedding and collection creation code\n"
      ],
      "metadata": {
        "id": "Len-nwUPRLV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8KAZMs1RCoD"
      },
      "outputs": [],
      "source": [
        "!pip install -q -r requirement_rag_refactored.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialise Copali and Qdrant"
      ],
      "metadata": {
        "id": "gYOq2FqASReS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.exceptions import UnexpectedResponse\n",
        "from colpali_engine.models import ColPali, ColPaliProcessor\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "\n",
        "#####################################################################\n",
        "#   Initializing Cloud Qdrant collection\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "\n",
        "# Replace these with your actual Cloud credentials\n",
        "QDRANT_URL = \"https://f7369634-b961-4d15-ba60-8b230e810658.us-east4-0.gcp.cloud.qdrant.io\"\n",
        "\n",
        "try:\n",
        "    # Initialize the Cloud Client\n",
        "    qdrant_client = QdrantClient(\n",
        "        url=QDRANT_URL,\n",
        "        api_key=userdata.get('QDRANT_API_KEY'),\n",
        "    )\n",
        "    print(\"Connected to Qdrant Cloud!\")\n",
        "except Exception as e:\n",
        "    print(f\"Cloud connection failed: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#   Initializing Qdrant collection from G Drive\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "# Initialize ColPali model and processor\n",
        "model_name = (\n",
        "    \"vidore/colpali-v1.2\"  # Use the latest version available\n",
        ")\n",
        "colpali_model = ColPali.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda:0\",  # Use \"cuda:0\" for GPU, \"cpu\" for CPU, or \"mps\" for Apple Silicon\n",
        ")\n",
        "colpali_processor = ColPaliProcessor.from_pretrained(\n",
        "    \"vidore/colpaligemma-3b-pt-448-base\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "xJ-RWyfGSrdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fn_Mqu58TG32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval and augmentation using Copali and gpt-4o"
      ],
      "metadata": {
        "id": "vDLsC0qlS9fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##### Full pipeline search and augment to get result\n",
        "\n",
        "\n",
        "import torch\n",
        "from qdrant_client import models\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "def run_visual_rag_full(query_text, colpali_model, colpali_processor, qdrant_client):\n",
        "    \"\"\"\n",
        "    Full pipeline:\n",
        "    1. Embed query (ColPali)\n",
        "    2. Search Qdrant (Multi-vector + Quantization)\n",
        "    3. Augment with GPT-4o Vision\n",
        "    \"\"\"\n",
        "\n",
        "    # --- STEP 1: ENCODE THE TEXT QUERY ---\n",
        "    with torch.no_grad():\n",
        "        # Process the query through ColPali\n",
        "        batch_query = colpali_processor.process_queries([query_text]).to(colpali_model.device)\n",
        "        # Convert the multi-vector output to a list of lists for Qdrant\n",
        "        query_embeddings = colpali_model(**batch_query).cpu().float().numpy().tolist()[0]\n",
        "\n",
        "    # --- STEP 2: SEARCH QDRANT CLOUD ---\n",
        "    # Using query_points to handle the Scalar Quantization and Multi-vector configuration\n",
        "    try:\n",
        "        search_result = qdrant_client.query_points(\n",
        "            collection_name=\"identity_documents\",\n",
        "            query=query_embeddings,\n",
        "            limit=1,\n",
        "            with_payload=True # Crucial to retrieve the 'base64_image' from disk\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Search failed: {e}\")\n",
        "        return None\n",
        "\n",
        "    if not search_result.points:\n",
        "        print(\"No matches found.\")\n",
        "        return None\n",
        "\n",
        "    # Extract top match and payload\n",
        "    top_hit = search_result.points[0]\n",
        "    base64_image = top_hit.payload.get('base64_image')\n",
        "    metadata = {\n",
        "        \"doc\": top_hit.payload.get('doc'),\n",
        "        \"page\": top_hit.payload.get('page')\n",
        "    }\n",
        "\n",
        "    print(f\"Match found! Source: {metadata['doc']}, Page: {metadata['page']}\")\n",
        "\n",
        "    # --- STEP 3: AUGMENT WITH GPT-4o VISION ---\n",
        "    # Initializing OpenAI client using your stored key\n",
        "    openai_client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant that answers questions based on the provided image context.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": f\"Question: {query_text}\"},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\n",
        "                            \"url\": f\"data:image/png;base64,{base64_image}\" # Using png as per your upsert format\n",
        "                        },\n",
        "                    },\n",
        "                ],\n",
        "            }\n",
        "        ],\n",
        "        max_tokens=500,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"answer\": response.choices[0].message.content,\n",
        "        \"metadata\": metadata\n",
        "    }\n",
        "\n",
        "# --- EXECUTION ---\n",
        "query = \"How old is Kaira\"\n",
        "result = run_visual_rag_full(query, colpali_model, colpali_processor, qdrant_client)\n",
        "\n",
        "if result:\n",
        "    print(\"\\n--- GPT-4o Response ---\")\n",
        "    print(result['answer'])\n"
      ],
      "metadata": {
        "id": "uya8LuOHTH9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}