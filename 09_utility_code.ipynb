{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0Cash3ygitu2o3Z8DhSoI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinaykrshnn-git2026/advanced-rag/blob/main/09_utility_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General Purpose Utility"
      ],
      "metadata": {
        "id": "WV4z5ySeWH-0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmmlutqAWBN2"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/vinaykrshnn-git2026/advanced-rag-refactored.git\n",
        "%cd advanced-rag-refactored\n",
        "!pip install -q -r requirement_rag_refactored.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GH3GFiUrS8pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q5dcMqxUf52B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy all pdf files from Drive to RAG Labs\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import fnmatch\n",
        "\n",
        "# Define source and destination root paths within your mounted Drive\n",
        "# Replace 'SourceFolder' and 'DestinationFolder' with your actual folder names/paths\n",
        "SRC_ROOT = '/content/drive/MyDrive'\n",
        "DEST_ROOT = '/content/drive/MyDrive/RAG_Labs/pdf_files'\n",
        "\n",
        "# 2. Create destination folder if it doesn't exist\n",
        "if not os.path.exists(DEST_ROOT):\n",
        "    os.makedirs(DEST_ROOT)\n",
        "    print(f\"Created folder: {DEST_ROOT}\")\n",
        "\n",
        "# 3. Recursive copy with flattening and skipping\n",
        "files_copied = 0\n",
        "files_skipped = 0\n",
        "\n",
        "for root, dirs, files in os.walk(SRC_ROOT):\n",
        "    for file in files:\n",
        "        if file.lower().endswith('.pdf'):\n",
        "            source_file_path = os.path.join(root, file)\n",
        "            destination_file_path = os.path.join(DEST_ROOT, file)\n",
        "\n",
        "            # Check if file exists to skip\n",
        "            if not os.path.exists(destination_file_path):\n",
        "                shutil.copy2(source_file_path, destination_file_path)\n",
        "                print(f\"Copied: {file}\")\n",
        "                files_copied += 1\n",
        "            else:\n",
        "                print(f\"Skipped (exists): {file}\")\n",
        "                files_skipped += 1\n",
        "\n",
        "print(f\"\\nSummary:\\nCopied: {files_copied}\\nSkipped: {files_skipped}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BkF7PzlKV9iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upsert a new image to existing Qdrant collection"
      ],
      "metadata": {
        "id": "MxI_S7Yp9kAL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WmcNfC1y-iII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.exceptions import UnexpectedResponse\n",
        "from colpali_engine.models import ColPali, ColPaliProcessor\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "\n",
        "#####################################################################\n",
        "#   Initializing Cloud Qdrant collection\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "from qdrant_client import QdrantClient\n",
        "\n",
        "# Replace these with your actual Cloud credentials\n",
        "QDRANT_URL = \"https://f7369634-b961-4d15-ba60-8b230e810658.us-east4-0.gcp.cloud.qdrant.io\"\n",
        "\n",
        "try:\n",
        "    # Initialize the Cloud Client\n",
        "    qdrant_client = QdrantClient(\n",
        "        url=QDRANT_URL,\n",
        "        api_key=userdata.get('QDRANT_API_KEY'),\n",
        "    )\n",
        "    print(\"Connected to Qdrant Cloud!\")\n",
        "except Exception as e:\n",
        "    print(f\"Cloud connection failed: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "#####################################################################\n",
        "#   Initializing Colpali\n",
        "#####################################################################\n",
        "\n",
        "\n",
        "# Initialize ColPali model and processor\n",
        "model_name = (\n",
        "    \"vidore/colpali-v1.2\"  # Use the latest version available\n",
        ")\n",
        "colpali_model = ColPali.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda:0\",  # Use \"cuda:0\" for GPU, \"cpu\" for CPU, or \"mps\" for Apple Silicon\n",
        ")\n",
        "colpali_processor = ColPaliProcessor.from_pretrained(\n",
        "    \"vidore/colpaligemma-3b-pt-448-base\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "xJ-RWyfGSrdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image\n",
        "import torch\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct\n",
        "import base64\n",
        "import uuid\n",
        "from io import BytesIO\n",
        "\n",
        "# 1. Load your image\n",
        "image_path = \"/content/drive/MyDrive/PHOTO-2022-01-12-09-24-39.jpg\"\n",
        "image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# 2. Encode to Base64 (to store in Qdrant payload so the UI can display it later)\n",
        "buffered = BytesIO()\n",
        "image.save(buffered, format=\"PNG\")\n",
        "base64_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "# 3. Generate ColPali Embeddings\n",
        "# Use the same processor/model currently in your 'models' dictionary\n",
        "with torch.no_grad():\n",
        "    batch_images = colpali_processor.process_images([image]).to(colpali_model.device)\n",
        "    image_embeddings = colpali_model(**batch_images)\n",
        "    # Convert to list for Qdrant and flatten the list\n",
        "    vector = image_embeddings.cpu().float().numpy().tolist()[0]\n",
        "\n",
        "\n",
        "# 4. Upsert to Qdrant\n",
        "unique_id = str(uuid.uuid4())\n",
        "try:\n",
        "      qdrant_client.upsert(\n",
        "          collection_name=\"identity_documents\",\n",
        "          points=[\n",
        "              PointStruct(\n",
        "                  id=unique_id, # A unique integer or UUID\n",
        "                  vector=vector,\n",
        "                  payload={\n",
        "                      \"doc\": \"manual_upload\",\n",
        "                      \"page\": 1,\n",
        "                      \"base64_image\": base64_string\n",
        "                  }\n",
        "              )\n",
        "          ]\n",
        "      )\n",
        "except Exception as e:\n",
        "                print(f\"Error during upsert: {e}\")"
      ],
      "metadata": {
        "id": "o9qU55Aa9tTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1BxjeytOg8Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTYJfFFT0pZx"
      },
      "outputs": [],
      "source": [
        "#####################################\n",
        "###### CODE TO DISPLAY IMAGES IN THE SEARCH RESULT   ###########\n",
        "######################################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from base64 import b64decode\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Extract the top images from the search result for display\n",
        "top_images = search_result.points[:6] # Adjust limit as needed, up to 6 for 2x3 grid\n",
        "\n",
        "# Determine number of images to display and create subplots\n",
        "num_images = len(top_images)\n",
        "if num_images == 0:\n",
        "    print(\"No images to display.\")\n",
        "else:\n",
        "    # Calculate rows and columns for subplot grid\n",
        "    cols = 3 # Max 3 columns for better readability\n",
        "    rows = (num_images + cols - 1) // cols # Calculate rows needed\n",
        "\n",
        "    fig, axs = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
        "    axs = axs.flatten() # Flatten the array for easy iteration\n",
        "\n",
        "    # Iterate over the top images and plot each one\n",
        "    for i, point in enumerate(top_images):\n",
        "        base64_string = point.payload.get('base64_image')\n",
        "        if base64_string:\n",
        "            image_data = b64decode(base64_string)\n",
        "            img = Image.open(BytesIO(image_data))\n",
        "            axs[i].imshow(img)\n",
        "            pdf_file = point.payload.get('doc', 'N/A')\n",
        "            page_num = point.payload.get('page', 'N/A')\n",
        "            axs[i].set_title(f\"Score: {point.score:.2f}\\nDoc: {pdf_file}, Page: {page_num}\")\n",
        "            axs[i].axis('off')  # Do not display axes for better visualization\n",
        "        else:\n",
        "            axs[i].set_title(\"Image not found\")\n",
        "            axs[i].axis('off')\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(i + 1, len(axs)):\n",
        "        fig.delaxes(axs[j])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vVL5cHbg_7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "###### CODE TO PRINT ALL DOCUMENTS IN COLLECTION   ###########\n",
        "######################################\n",
        "\n",
        "def get_unique_doc_names(collection_name=\"rag_documents_v2\"):\n",
        "    doc_names = set()\n",
        "    next_page_offset = None\n",
        "\n",
        "    while True:\n",
        "        # Scroll through the collection\n",
        "        points, next_page_offset = qdrant_client.scroll(\n",
        "            collection_name=collection_name,\n",
        "            limit=100, # Adjust batch size as needed\n",
        "            with_payload=[\"doc\"], # Only fetch the 'doc' field to save bandwidth\n",
        "            with_vectors=False,   # We don't need the vectors for this\n",
        "            offset=next_page_offset\n",
        "        )\n",
        "\n",
        "        for point in points:\n",
        "            if \"doc\" in point.payload:\n",
        "                doc_names.add(point.payload[\"doc\"])\n",
        "\n",
        "        # If next_page_offset is None, we've reached the end\n",
        "        if next_page_offset is None:\n",
        "            break\n",
        "\n",
        "    return list(doc_names)\n",
        "\n",
        "# Usage\n",
        "unique_docs = get_unique_doc_names()\n",
        "print(f\"Documents found: {unique_docs}\")"
      ],
      "metadata": {
        "id": "FqzjN1brUSID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yCAHKWlqhFvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "###### CODE TO UPSERT A NEW PDF INTO AN EXISTING COLLECTION   ###########\n",
        "######################################\n",
        "\n",
        "import torch\n",
        "from pdf2image import convert_from_path\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from colpali_engine.models import ColPali\n",
        "#from colpali_engine.processor import ColPaliProcessor\n",
        "from colpali_engine.models import ColPali, ColPaliProcessor\n",
        "import base64\n",
        "import uuid\n",
        "from io import BytesIO\n",
        "\n",
        "def process_and_upsert_pdf(pdf_path: str):\n",
        "    # 2. Convert PDF to Images (Generates one PIL image per page)\n",
        "    # Using 150 DPI is a good balance for ColPali\n",
        "    images = convert_from_path(pdf_path, dpi=150)\n",
        "\n",
        "    points = []\n",
        "\n",
        "    for i, image in enumerate(images):\n",
        "        page_num = i + 1\n",
        "        print(f\"Processing page {page_num}...\")\n",
        "\n",
        "        # 3. Generate ColPali Embeddings\n",
        "        with torch.no_grad():\n",
        "            batch_images = colpali_processor.process_images([image]).to(colpali_model.device)\n",
        "            image_embeddings = colpali_model(**batch_images)\n",
        "\n",
        "            #Access the attention mask\n",
        "            mask = batch_images.attention_mask\n",
        "            # Remove batch dim and move to CPU for Qdrant\n",
        "            #multivector = image_embeddings[0].cpu().float().numpy().tolist()\n",
        "\n",
        "        # 2. Prepare points with Base64 payloads\n",
        "            points = []\n",
        "\n",
        "            # If image_embeddings is a list, we iterate through it directly\n",
        "            for j, embedding in enumerate(image_embeddings):\n",
        "                # Determine the number of non-padding tokens for this specific image\n",
        "                actual_num_patches = mask[j].sum().item()\n",
        "\n",
        "                # Filter the embedding to only include 'real' visual patches before converting to list\n",
        "                # This prevents 'diluting' the search score with empty padding vectors\n",
        "                filtered_embedding = embedding[:actual_num_patches].cpu().float().numpy().tolist()\n",
        "\n",
        "                # --- Convert PIL image to Base64 string ---\n",
        "                buffered = BytesIO()\n",
        "                images[j].save(buffered, format=\"PNG\")\n",
        "                base64_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "                unique_id = str(uuid.uuid4())\n",
        "\n",
        "                points.append(\n",
        "                    models.PointStruct(\n",
        "                        id=unique_id,\n",
        "                        vector=filtered_embedding, # Use the filtered multivector\n",
        "                        payload={\n",
        "                            \"doc\": \"Ishya_Passport\",\n",
        "                            \"page\": i + j + 1,\n",
        "                            \"base64_image\": base64_string\n",
        "                        },\n",
        "                    )\n",
        "                )\n",
        "\n",
        "           # 3. Upsert to Qdrant Cloud\n",
        "            try:\n",
        "                qdrant_client.upsert(\n",
        "                    collection_name=\"rag_documents_v2\",\n",
        "                    points=points\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error during upsert: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"Finished upserting {pdf_path}\")\n",
        "\n",
        "# Run the pipeline\n",
        "process_and_upsert_pdf(\"/content/advanced-rag/data/Ishya Passport 2026.pdf\")\n"
      ],
      "metadata": {
        "id": "KMbJDm5rcXW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5smUPVWZhJDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################\n",
        "###### CODE TO UPSERT A NEW IMAGE INTO AN EXISTING COLLECTION   ###########\n",
        "######################################\n",
        "\n",
        "import PIL.Image\n",
        "import torch\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import PointStruct\n",
        "import base64\n",
        "import uuid\n",
        "from io import BytesIO\n",
        "\n",
        "# 1. Load your image\n",
        "image_path = \"/content/drive/MyDrive/RAG_Labs/pdf_files/medicare_files/savi_medicare_img.jpg\"\n",
        "image = PIL.Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "# 2. Encode to Base64 (to store in Qdrant payload so the UI can display it later)\n",
        "buffered = BytesIO()\n",
        "image.save(buffered, format=\"PNG\")\n",
        "base64_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "# 3. Generate ColPali Embeddings\n",
        "# Use the same processor/model currently in your 'models' dictionary\n",
        "with torch.no_grad():\n",
        "    batch_images = colpali_processor.process_images([image]).to(colpali_model.device)\n",
        "    image_embeddings = colpali_model(**batch_images)\n",
        "    # Convert to list for Qdrant and flatten the list\n",
        "    vector = image_embeddings.cpu().float().numpy().tolist()[0]\n",
        "\n",
        "\n",
        "# 4. Upsert to Qdrant\n",
        "unique_id = str(uuid.uuid4())\n",
        "try:\n",
        "      qdrant_client.upsert(\n",
        "          collection_name=\"rag_documents_v2\",\n",
        "          points=[\n",
        "              PointStruct(\n",
        "                  id=unique_id, # A unique integer or UUID\n",
        "                  vector=vector,\n",
        "                  payload={\n",
        "                      \"doc\": \"manual_upload\",\n",
        "                      \"page\": 1,\n",
        "                      \"base64_image\": base64_string\n",
        "                  }\n",
        "              )\n",
        "          ]\n",
        "      )\n",
        "except Exception as e:\n",
        "                print(f\"Error during upsert: {e}\")"
      ],
      "metadata": {
        "id": "OeYqkRGQGyMN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}